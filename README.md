# ğŸ“Š SCR Credit Analytics: Pipeline de Engenharia e VisualizaÃ§Ã£o

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-FFF000?style=for-the-badge&logo=duckdb&logoColor=black)
![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Plotly](https://img.shields.io/badge/Plotly-239120?style=for-the-badge&logo=plotly&logoColor=white)

## ğŸ“Œ VisÃ£o Executiva
Este projeto Ã© uma soluÃ§Ã£o *end-to-end* de **Analytics Engineering** voltada para o acompanhamento do mercado de crÃ©dito brasileiro. Utilizando dados pÃºblicos do **Sistema de InformaÃ§Ãµes de CrÃ©dito (SCR)** do Banco Central do Brasil, a aplicaÃ§Ã£o processa grandes volumes de dados brutos e os converte em um painel executivo focado em tomada de decisÃ£o estratÃ©gica, gestÃ£o de portfÃ³lio e anÃ¡lise de risco (inadimplÃªncia/Over90).

## ğŸ—ï¸ Arquitetura e Fluxo de Dados (Modern Data Stack)

A arquitetura foi desenhada priorizando performance, modularidade e isolamento de responsabilidades:

1. **Extract (Python):** Script autÃ´nomo (`extract.py`) que faz requisiÃ§Ãµes ao portal de dados abertos do BCB, processa os arquivos ZIP de safras especÃ­ficas (2024 a 2026) e converte os relatÃ³rios CSV em arquivos `.parquet`. Essa abordagem reduz drasticamente o tempo de I/O e o custo de armazenamento.
2. **Transform (dbt + DuckDB):** Modelagem dimensional focada em regras de negÃ³cio. O DuckDB atua como *engine* OLAP em memÃ³ria, garantindo alta velocidade em consultas analÃ­ticas.
   * **Staging (`stg_scr_data`):** HigienizaÃ§Ã£o de dados, tipagem forte (cast de *strings* para *dates/floats*) e padronizaÃ§Ã£o.
   * **Marts (`mart_scr_*`):** Tabelas agregadas que entregam valor direto para a visualizaÃ§Ã£o, lidando com cÃ¡lculos complexos de VariaÃ§Ã£o Mensal (MoM) e representatividade em pontos percentuais (p.p.).
3. **Serve (Streamlit):** Interface *web* de alto nÃ­vel, com grÃ¡ficos desenvolvidos em `plotly.graph_objects` para controle preciso de eixos duplos, labels customizados (Mi/Bi/Tri) e padrÃµes de formataÃ§Ã£o executiva.
4. **Deploy (Docker):** Ambiente 100% conteinerizado (via `docker-compose`), garantindo que o pipeline seja reproduzÃ­vel em qualquer mÃ¡quina ou servidor.

## ğŸ“‚ Estrutura de DiretÃ³rios

```text
â”œâ”€â”€ app/
â”‚   â””â”€â”€ dashboard.py               # Front-end da aplicaÃ§Ã£o e lÃ³gicas de UI/UX
â”œâ”€â”€ dbt_scr/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Camada Prata: Tratamento e limpeza
â”‚   â”‚   â””â”€â”€ marts/                 # Camada Ouro: Tabelas consolidadas por dimensÃ£o
â”‚   â”œâ”€â”€ dbt_project.yml            # ConfiguraÃ§Ãµes gerais do projeto dbt
â”‚   â””â”€â”€ profiles.yml               # Mapeamento da conexÃ£o dbt <> DuckDB
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ extract.py                 # OrquestraÃ§Ã£o do web scraping e geraÃ§Ã£o de Parquets
â”œâ”€â”€ .gitignore                     # ProteÃ§Ã£o contra vazamento de dados locais/bancos
â”œâ”€â”€ docker-compose.yml             # OrquestraÃ§Ã£o de serviÃ§os
â”œâ”€â”€ Dockerfile                     # ConstruÃ§Ã£o da imagem do ambiente Python/Streamlit
â””â”€â”€ requirements.txt               # Trava de dependÃªncias (incluindo Streamlit >=1.36)
```

## ğŸ“ˆ Funcionalidades do Dashboard
O painel foi estruturado para refletir as necessidades de uma diretoria de PolÃ­ticas de CrÃ©dito ou MIS:

**Big Numbers:** Cards de resumo mostrando o Volume Total de Carteira Ativa e o Ãndice de InadimplÃªncia, comparando o mÃªs atual com o mÃªs anterior (cÃ¡lculos de variaÃ§Ã£o relativa e variaÃ§Ã£o absoluta em p.p.).

**TendÃªncia Macro:** GrÃ¡fico histÃ³rico (Line Chart) cruzando a evoluÃ§Ã£o da InadimplÃªncia vs. Ativos ProblemÃ¡ticos.

**Mapa de Risco GeogrÃ¡fico:** AnÃ¡lise cross-section com grÃ¡fico de barras horizontais, ordenado e com escala de cor (degradÃª de risco) mapeando o comportamento de crÃ©dito por Unidade Federativa (UF).

**SegmentaÃ§Ã£o PF x PJ:** Monitoramento histÃ³rico comparativo entre linhas de negÃ³cio (Pessoa FÃ­sica e JurÃ­dica).

**Performance por Modalidade:** GrÃ¡fico de Pareto avanÃ§ado (Eixos Duplos) cruzando o Saldo Contratado (Barras com formataÃ§Ã£o inteligente R$ Bi/Tri) e o Risco Associado (Linha de InadimplÃªncia).

**Perfil de Endividamento:** AnÃ¡lise de Curto vs. Longo Prazo estratificada por Faixa de Rendimento/Porte.

## âš™ï¸ Como Executar o Projeto Localmente
Para rodar este projeto na sua mÃ¡quina, siga os passos abaixo:

**1. PrÃ©-requisitos**
**Python 3.10+** instalado

**Docker** e **Docker Compose** instalados e rodando

**2. ExtraÃ§Ã£o dos Dados (Carga Inicial)**
Por seguranÃ§a e boas prÃ¡ticas, o repositÃ³rio nÃ£o hospeda os bancos de dados reais. VocÃª precisarÃ¡ extrair os dados do BCB rodando o script:

_Bash_
```text
python scripts/extract.py
```
(Este comando farÃ¡ o download das safras de 2024 a 2026 e gerarÃ¡ a pasta de arquivos Parquet localmente).

**3. ExecuÃ§Ã£o das TransformaÃ§Ãµes (dbt)**

Com os dados extraÃ­dos, construa as tabelas analÃ­ticas (.duckdb):

_Bash_
```text
cd dbt_scr
dbt deps      # Baixa os pacotes necessÃ¡rios
dbt build     # Executa testes e materializa as models
```
**4. Subindo a AplicaÃ§Ã£o Web**

Retorne Ã  raiz do projeto e inicie o container Docker:

_Bash_
```text
cd ..
docker-compose up -d --build
```
Acesse o painel em seu navegador atravÃ©s de: http://localhost:8501

Desenvolvido com foco em escalabilidade de dados, governanÃ§a e clareza analÃ­tica.
